{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFS Experiments: RAG Performance Analysis\n",
    "\n",
    "This notebook conducts systematic experiments with Google Generative File Search.\n",
    "\n",
    "**Objectives**:\n",
    "1. Define test query set\n",
    "2. Measure response latency (P50, P95, P99)\n",
    "3. Analyze retrieval quality\n",
    "4. Examine grounding/citations\n",
    "5. Calculate costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from gfs_client import GFSClient\n",
    "from utils import load_api_key\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Store and Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "api_key = load_api_key(\"GOOGLE_API_KEY\", str(project_root / \".env\"))\n",
    "gfs = GFSClient(api_key=api_key)\n",
    "\n",
    "# Load store metadata\n",
    "metadata_path = project_root / \"models\" / \"gfs_stores\" / \"metadata.json\"\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    store_metadata = json.load(f)\n",
    "\n",
    "store_name = store_metadata[\"store_name\"]\n",
    "print(f\"Using store: {store_metadata['display_name']}\")\n",
    "print(f\"Store name: {store_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Test Queries\n",
    "\n",
    "Create a diverse query set covering different patterns:\n",
    "- Factual lookups\n",
    "- Analytical questions\n",
    "- Multi-document reasoning\n",
    "- Edge cases (out-of-domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query set\n",
    "test_queries = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"query\": \"What are the main topics covered in the documents?\",\n",
    "        \"category\": \"overview\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"query\": \"Summarize the key findings or conclusions.\",\n",
    "        \"category\": \"synthesis\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q3\",\n",
    "        \"query\": \"What specific data or statistics are mentioned?\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q4\",\n",
    "        \"query\": \"Are there any recommendations or best practices?\",\n",
    "        \"category\": \"actionable\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q5\",\n",
    "        \"query\": \"What is the weather forecast for tomorrow?\",\n",
    "        \"category\": \"out_of_domain\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Test queries: {len(test_queries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run queries and collect metrics\n",
    "results = []\n",
    "\n",
    "for i, test in enumerate(test_queries):\n",
    "    print(f\"\\n[{i+1}/{len(test_queries)}] {test['id']}: {test['query'][:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = gfs.query_with_file_search(\n",
    "            query=test[\"query\"],\n",
    "            store_names=[store_name],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Extract citations\n",
    "        citations = gfs.extract_citations(response)\n",
    "        has_citations = citations is not None and citations.get(\"search_entry_point\") is not None\n",
    "        \n",
    "        # Count tokens (rough estimate)\n",
    "        response_text = response.text\n",
    "        token_count = len(response_text.split()) * 1.3  # Rough approximation\n",
    "        \n",
    "        result = {\n",
    "            \"query_id\": test[\"id\"],\n",
    "            \"category\": test[\"category\"],\n",
    "            \"latency_seconds\": latency,\n",
    "            \"response_length\": len(response_text),\n",
    "            \"estimated_tokens\": int(token_count),\n",
    "            \"has_citations\": has_citations,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Latency: {latency:.2f}s, Citations: {has_citations}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        result = {\n",
    "            \"query_id\": test[\"id\"],\n",
    "            \"category\": test[\"category\"],\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        print(f\"  ✗ Failed: {e}\")\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nExperiments completed: {len(results)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate latency statistics\n",
    "successful_results = [r for r in results if r[\"status\"] == \"success\"]\n",
    "\n",
    "if successful_results:\n",
    "    latencies = [r[\"latency_seconds\"] for r in successful_results]\n",
    "    \n",
    "    print(\"Latency Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(latencies):.3f}s\")\n",
    "    print(f\"  Median (P50): {np.median(latencies):.3f}s\")\n",
    "    print(f\"  P95: {np.percentile(latencies, 95):.3f}s\")\n",
    "    print(f\"  P99: {np.percentile(latencies, 99):.3f}s\")\n",
    "    print(f\"  Min: {np.min(latencies):.3f}s\")\n",
    "    print(f\"  Max: {np.max(latencies):.3f}s\")\n",
    "    \n",
    "    # Citation rate\n",
    "    citation_rate = sum(r[\"has_citations\"] for r in successful_results) / len(successful_results)\n",
    "    print(f\"\\nCitation Rate: {citation_rate*100:.1f}%\")\n",
    "else:\n",
    "    print(\"No successful queries to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency distribution\n",
    "if successful_results:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    ax1.hist(latencies, bins=10, edgecolor=\"black\", alpha=0.7)\n",
    "    ax1.axvline(np.median(latencies), color=\"red\", linestyle=\"--\", label=\"Median\")\n",
    "    ax1.set_xlabel(\"Latency (seconds)\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "    ax1.set_title(\"GFS Query Latency Distribution\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Box plot by category\n",
    "    categories = [r[\"category\"] for r in successful_results]\n",
    "    unique_cats = list(set(categories))\n",
    "    \n",
    "    cat_latencies = [[r[\"latency_seconds\"] for r in successful_results if r[\"category\"] == cat] \n",
    "                     for cat in unique_cats]\n",
    "    \n",
    "    ax2.boxplot(cat_latencies, labels=unique_cats)\n",
    "    ax2.set_ylabel(\"Latency (seconds)\")\n",
    "    ax2.set_title(\"Latency by Query Category\")\n",
    "    ax2.tick_params(axis=\"x\", rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate query costs\n",
    "if successful_results:\n",
    "    total_tokens = sum(r[\"estimated_tokens\"] for r in successful_results)\n",
    "    \n",
    "    # Gemini pricing (example rates)\n",
    "    # Input: retrieved context counted as input tokens\n",
    "    # Output: generated response tokens\n",
    "    # Using approximate rate: $0.075 per 1M input tokens, $0.30 per 1M output tokens\n",
    "    \n",
    "    estimated_cost_per_query = (total_tokens / 1_000_000) * 0.30 / len(successful_results)\n",
    "    \n",
    "    print(\"Cost Estimation:\")\n",
    "    print(f\"  Total estimated tokens: {total_tokens:,}\")\n",
    "    print(f\"  Avg tokens per query: {total_tokens / len(successful_results):.0f}\")\n",
    "    print(f\"  Estimated cost per query: ${estimated_cost_per_query:.6f}\")\n",
    "    print(f\"  Estimated cost for 1000 queries: ${estimated_cost_per_query * 1000:.2f}\")\n",
    "    print(\"\\nNote: This is a rough estimate. Actual costs depend on retrieved context size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experimental results\n",
    "results_path = project_root / \"models\" / \"gfs_stores\" / \"experiment_results.json\"\n",
    "\n",
    "output = {\n",
    "    \"store_name\": store_name,\n",
    "    \"model_id\": gfs.model_id,\n",
    "    \"queries\": test_queries,\n",
    "    \"results\": results,\n",
    "    \"summary\": {\n",
    "        \"total_queries\": len(results),\n",
    "        \"successful\": len(successful_results),\n",
    "        \"failed\": len(results) - len(successful_results),\n",
    "        \"mean_latency\": np.mean(latencies) if successful_results else None,\n",
    "        \"median_latency\": np.median(latencies) if successful_results else None,\n",
    "        \"citation_rate\": citation_rate if successful_results else None,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**GFS Performance Metrics Collected**:\n",
    "- Query latency (P50, P95, P99)\n",
    "- Citation/grounding rate\n",
    "- Token usage estimates\n",
    "- Cost projections\n",
    "\n",
    "**Next Steps**:\n",
    "- Implement custom RAG baseline in `04_custom_rag_baseline.ipynb`\n",
    "- Run same query set on custom RAG\n",
    "- Compare metrics in `05_comparison_analysis.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
