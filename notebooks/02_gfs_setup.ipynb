{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFS Setup: File Search Store Creation\n",
    "\n",
    "This notebook sets up Google Generative File Search (GFS) for the RAG comparison.\n",
    "\n",
    "**Objectives**:\n",
    "1. Initialize GFS client with API key\n",
    "2. Create file search store\n",
    "3. Upload documents from `data/raw/`\n",
    "4. Verify indexing completion\n",
    "5. Test basic queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from gfs_client import GFSClient\n",
    "from data_loader import scan_documents, check_gfs_compatibility\n",
    "from utils import load_api_key\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize GFS Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from .env\n",
    "api_key = load_api_key(\"GOOGLE_API_KEY\", str(project_root / \".env\"))\n",
    "\n",
    "# Initialize client\n",
    "gfs = GFSClient(api_key=api_key, model_id=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "print(\"GFS client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Existing Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing stores\n",
    "existing_stores = gfs.list_stores()\n",
    "\n",
    "print(f\"Existing stores: {len(existing_stores)}\")\n",
    "for store in existing_stores:\n",
    "    print(f\"  - {store.display_name}: {store.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create New File Search Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create store\n",
    "store_display_name = \"RAG Comparison Document Store\"\n",
    "\n",
    "store = gfs.create_file_search_store(display_name=store_display_name)\n",
    "\n",
    "print(f\"Created store: {store.display_name}\")\n",
    "print(f\"Store name: {store.name}\")\n",
    "print(f\"Created at: {store.create_time}\")\n",
    "\n",
    "# Save store metadata\n",
    "store_metadata = {\n",
    "    \"display_name\": store.display_name,\n",
    "    \"store_name\": store.name,\n",
    "    \"create_time\": str(store.create_time),\n",
    "}\n",
    "\n",
    "metadata_path = project_root / \"models\" / \"gfs_stores\" / \"metadata.json\"\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(store_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scan and Upload Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan documents\n",
    "data_dir = project_root / \"data\" / \"raw\"\n",
    "df = scan_documents(data_dir)\n",
    "\n",
    "print(f\"Total files found: {len(df)}\")\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"\\nNo files found in data/raw/\")\n",
    "    print(\"Add documents to continue.\")\n",
    "else:\n",
    "    # Check compatibility\n",
    "    df_compat = check_gfs_compatibility(df)\n",
    "    compatible_files = df_compat.filter(pl.col(\"gfs_compatible\"))\n",
    "    \n",
    "    print(f\"Compatible files: {len(compatible_files)}\")\n",
    "    print(f\"Incompatible files: {len(df) - len(compatible_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload compatible files to store\n",
    "if len(df) > 0 and len(compatible_files) > 0:\n",
    "    upload_results = []\n",
    "    \n",
    "    for i, row in enumerate(compatible_files.iter_rows(named=True)):\n",
    "        file_path = Path(row[\"file_path\"])\n",
    "        print(f\"\\nUploading {i+1}/{len(compatible_files)}: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            operation = gfs.upload_to_store(\n",
    "                store_name=store.name,\n",
    "                file_path=file_path,\n",
    "                wait_for_completion=True\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            upload_results.append({\n",
    "                \"file_name\": file_path.name,\n",
    "                \"status\": \"success\",\n",
    "                \"upload_time_seconds\": elapsed\n",
    "            })\n",
    "            \n",
    "            print(f\"  ✓ Uploaded successfully ({elapsed:.1f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            upload_results.append({\n",
    "                \"file_name\": file_path.name,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            print(f\"  ✗ Failed: {e}\")\n",
    "    \n",
    "    # Save upload results\n",
    "    results_path = project_root / \"models\" / \"gfs_stores\" / \"upload_results.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(upload_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nUpload results saved to: {results_path}\")\n",
    "else:\n",
    "    print(\"No compatible files to upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Store Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get updated store info\n",
    "store_info = gfs.get_store_info(store.name)\n",
    "\n",
    "print(\"Store Status:\")\n",
    "print(f\"  Display name: {store_info.display_name}\")\n",
    "print(f\"  Size: {store_info.size_bytes / (1024*1024):.2f} MB\")\n",
    "print(f\"  Active documents: {store_info.active_documents_count}\")\n",
    "print(f\"  Pending documents: {store_info.pending_documents_count}\")\n",
    "print(f\"  Failed documents: {store_info.failed_documents_count}\")\n",
    "print(f\"  Last update: {store_info.update_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query (only if documents are uploaded)\n",
    "if len(df) > 0 and len(compatible_files) > 0:\n",
    "    test_query = \"What are the main topics covered in these documents?\"\n",
    "    \n",
    "    print(f\"Test query: {test_query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    response = gfs.query_with_file_search(\n",
    "        query=test_query,\n",
    "        store_names=[store.name],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResponse:\\n{response.text}\")\n",
    "    \n",
    "    # Check for citations\n",
    "    citations = gfs.extract_citations(response)\n",
    "    if citations:\n",
    "        print(\"\\n[Sources cited from documents]\")\n",
    "    else:\n",
    "        print(\"\\n[No citations found]\")\n",
    "else:\n",
    "    print(\"Skipping test query - no documents uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Completed**:\n",
    "- Created GFS file search store\n",
    "- Uploaded compatible documents\n",
    "- Verified indexing status\n",
    "- Tested basic query functionality\n",
    "\n",
    "**Next Steps**:\n",
    "- Proceed to `03_gfs_experiments.ipynb` for detailed RAG experiments\n",
    "- Test various query patterns\n",
    "- Measure latency and retrieval quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
