{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración de GFS: Creación del Almacén de Búsqueda de Archivos\n",
    "\n",
    "Este cuaderno configura Google Generative File Search (GFS) para la comparación de RAG.\n",
    "\n",
    "**Objetivos**:\n",
    "1. Inicializar el cliente GFS con la clave API\n",
    "2. Crear el almacén de búsqueda de archivos\n",
    "3. Cargar documentos desde `data/raw/`\n",
    "4. Verificar la finalización de la indexación\n",
    "5. Probar consultas básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Force reload of modules\n",
    "import importlib\n",
    "if 'gfs_client' in sys.modules:\n",
    "    importlib.reload(sys.modules['gfs_client'])\n",
    "if 'data_loader' in sys.modules:\n",
    "    importlib.reload(sys.modules['data_loader'])\n",
    "if 'utils' in sys.modules:\n",
    "    importlib.reload(sys.modules['utils'])\n",
    "\n",
    "from gfs_client import GFSClient\n",
    "from data_loader import scan_documents, check_gfs_compatibility\n",
    "from utils import load_api_key\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "print(\"Imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicializar Cliente GFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFS client initialized\n"
     ]
    }
   ],
   "source": [
    "# Load API key from .env\n",
    "api_key = load_api_key(\"GOOGLE_API_KEY\", str(project_root / \".env\"))\n",
    "\n",
    "# Initialize client\n",
    "gfs = GFSClient(api_key=api_key, model_id=\"gemini-2.5-flash\")\n",
    "\n",
    "print(\"GFS client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verificar Almacenes Existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing stores: 19\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-bl56voi03s98\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-vjeb3eavqtut\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-v68rdv4540e0\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-7040d5gdzax3\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-apaqaior2z2o\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-mmidib7ivotz\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-wb9kd8me89wn\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-5ail0a2oviht\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-blmaj7gsmtit\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-0lorkz3md71l\n",
      "  - RAG Comparison Document Store: fileSearchStores/rag-comparison-document-sto-o05ylcrcl2f6\n",
      "  - Politicas Corporativas: fileSearchStores/politicas-corporativas-l2nnohr1ems1\n",
      "  - Politicas Corporativas: fileSearchStores/politicas-corporativas-v8czleq6bwe9\n",
      "  - Politicas Corporativas: fileSearchStores/politicas-corporativas-acqu6osv3o37\n",
      "  - Politicas Corporativas: fileSearchStores/politicas-corporativas-vsejtgxd9i8y\n",
      "  - Politicas Corporativas: fileSearchStores/politicas-corporativas-z9x83ql37cub\n",
      "  - Politicas Corporativas: fileSearchStores/politicas-corporativas-m6y97n13k2g9\n",
      "  - Politicas Corporativas: fileSearchStores/politicas-corporativas-gg2fdcjno21x\n",
      "  - RAG Verification Store: fileSearchStores/rag-verification-store-shq5pg53cs7s\n"
     ]
    }
   ],
   "source": [
    "# List existing stores\n",
    "existing_stores = gfs.list_stores()\n",
    "\n",
    "print(f\"Existing stores: {len(existing_stores)}\")\n",
    "for store in existing_stores:\n",
    "    print(f\"  - {store.display_name}: {store.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear Nuevo Almacén de Búsqueda de Archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing store: RAG Comparison Document Store\n",
      "Store name: fileSearchStores/rag-comparison-document-sto-bl56voi03s98\n",
      "\n",
      "Metadata saved to: /Users/ggoni/docencia-repos/rag-with-gfs/models/gfs_stores/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Use existing store or create new one\n",
    "store_display_name = \"RAG Comparison Document Store\"\n",
    "\n",
    "# Check if we already have stores\n",
    "if len(existing_stores) > 0:\n",
    "    # Use the first existing store\n",
    "    store = existing_stores[0]\n",
    "    print(f\"Using existing store: {store.display_name}\")\n",
    "    print(f\"Store name: {store.name}\")\n",
    "else:\n",
    "    # Create new store\n",
    "    store = gfs.create_file_search_store(display_name=store_display_name)\n",
    "    print(f\"Created store: {store.display_name}\")\n",
    "    print(f\"Store name: {store.name}\")\n",
    "\n",
    "# Save store metadata\n",
    "store_metadata = {\n",
    "    \"display_name\": store.display_name,\n",
    "    \"store_name\": store.name,\n",
    "}\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "metadata_dir = project_root / \"models\" / \"gfs_stores\"\n",
    "metadata_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata_path = metadata_dir / \"metadata.json\"\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(store_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetadata saved to: {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Escanear y Cargar Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 4\n",
      "Compatible files: 4\n",
      "Incompatible files: 0\n"
     ]
    }
   ],
   "source": [
    "# Scan documents\n",
    "data_dir = project_root / \"data\" / \"raw\"\n",
    "df = scan_documents(data_dir)\n",
    "\n",
    "print(f\"Total files found: {len(df)}\")\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"\\nNo files found in data/raw/\")\n",
    "    print(\"Add documents to continue.\")\n",
    "else:\n",
    "    # Check compatibility\n",
    "    df_compat = check_gfs_compatibility(df)\n",
    "    compatible_files = df_compat.filter(pl.col(\"gfs_compatible\"))\n",
    "    \n",
    "    print(f\"Compatible files: {len(compatible_files)}\")\n",
    "    print(f\"Incompatible files: {len(df) - len(compatible_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading 1/4: data_science_workflow.txt\n",
      "  ✓ Uploaded successfully (6.0s)\n",
      "\n",
      "Uploading 2/4: nlp_overview.txt\n",
      "  ✓ Uploaded successfully (5.7s)\n",
      "\n",
      "Uploading 3/4: deep_learning_guide.txt\n",
      "  ✓ Uploaded successfully (5.3s)\n",
      "\n",
      "Uploading 4/4: ml_fundamentals.txt\n",
      "  ✓ Uploaded successfully (5.6s)\n",
      "\n",
      "Upload results saved to: /Users/ggoni/docencia-repos/rag-with-gfs/models/gfs_stores/upload_results.json\n"
     ]
    }
   ],
   "source": [
    "# Upload compatible files to store\n",
    "if len(df) > 0 and len(compatible_files) > 0:\n",
    "    upload_results = []\n",
    "    \n",
    "    for i, row in enumerate(compatible_files.iter_rows(named=True)):\n",
    "        file_path = Path(row[\"file_path\"])\n",
    "        print(f\"\\nUploading {i+1}/{len(compatible_files)}: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            operation = gfs.upload_to_store(\n",
    "                store_name=store.name,\n",
    "                file_path=file_path,\n",
    "                wait_for_completion=True\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            upload_results.append({\n",
    "                \"file_name\": file_path.name,\n",
    "                \"status\": \"success\",\n",
    "                \"upload_time_seconds\": elapsed\n",
    "            })\n",
    "            \n",
    "            print(f\"  ✓ Uploaded successfully ({elapsed:.1f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            upload_results.append({\n",
    "                \"file_name\": file_path.name,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            print(f\"  ✗ Failed: {e}\")\n",
    "    \n",
    "    # Save upload results\n",
    "    results_path = project_root / \"models\" / \"gfs_stores\" / \"upload_results.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(upload_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nUpload results saved to: {results_path}\")\n",
    "else:\n",
    "    print(\"No compatible files to upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verificar Estado del Almacén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store Status:\n",
      "  Display name: RAG Comparison Document Store\n",
      "  Size: 0.12 MB\n",
      "  Active documents: 20\n",
      "  Pending documents: None\n",
      "  Failed documents: None\n",
      "  Last update: 2025-12-01 00:02:12.733951+00:00\n"
     ]
    }
   ],
   "source": [
    "# Get updated store info\n",
    "store_info = gfs.get_store_info(store.name)\n",
    "\n",
    "print(\"Store Status:\")\n",
    "print(f\"  Display name: {store_info.display_name}\")\n",
    "\n",
    "# Handle optional attributes safely\n",
    "if hasattr(store_info, 'size_bytes') and store_info.size_bytes:\n",
    "    print(f\"  Size: {store_info.size_bytes / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"  Size: N/A\")\n",
    "\n",
    "if hasattr(store_info, 'active_documents_count'):\n",
    "    print(f\"  Active documents: {store_info.active_documents_count}\")\n",
    "else:\n",
    "    print(f\"  Active documents: N/A\")\n",
    "\n",
    "if hasattr(store_info, 'pending_documents_count'):\n",
    "    print(f\"  Pending documents: {store_info.pending_documents_count}\")\n",
    "else:\n",
    "    print(f\"  Pending documents: N/A\")\n",
    "\n",
    "if hasattr(store_info, 'failed_documents_count'):\n",
    "    print(f\"  Failed documents: {store_info.failed_documents_count}\")\n",
    "else:\n",
    "    print(f\"  Failed documents: N/A\")\n",
    "\n",
    "if hasattr(store_info, 'update_time') and store_info.update_time:\n",
    "    print(f\"  Last update: {store_info.update_time}\")\n",
    "else:\n",
    "    print(f\"  Last update: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Consulta de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test query: What are the main topics covered in these documents?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "The documents primarily cover the field of Natural Language Processing (NLP), detailing its fundamental concepts, methods, and core tasks.\n",
      "\n",
      "The main topics include:\n",
      "*   **Introduction to NLP**: Defining NLP as a branch of artificial intelligence that enables computers to understand, interpret, and generate human language, combining computational linguistics, machine learning, and deep learning.\n",
      "*   **Fundamental Concepts**:\n",
      "    *   **Text Preprocessing**: Essential steps before modeling, such as tokenization, lowercasing, stop word removal, stemming/lemmatization, and handling punctuation, special characters, numbers, and dates.\n",
      "    *   **Text Representation**: Both traditional methods like Bag of Words (BoW), TF-IDF, N-grams, Word2Vec, GloVe, and FastText, as well as modern methods such as contextual embeddings (BERT, ELMo, GPT), sentence embeddings (Sentence-BERT, Universal Sentence Encoder), and subword tokenization (BPE, WordPiece, SentencePiece).\n",
      "*   **Core NLP Tasks**:\n",
      "    *   **Text Classification**: Assigning categories to documents or sentences, including sentiment analysis, topic classification, intent detection, and spam detection, using approaches like Naive Bayes, Logistic Regression, CNNs, RNNs, and Transformers.\n",
      "    *   **Named Entity Recognition (NER)**: Identifying and classifying entities in text, such as person names, organizations, locations, and dates.\n",
      "\n",
      "[Sources cited from documents]\n"
     ]
    }
   ],
   "source": [
    "# Test query (only if documents are uploaded)\n",
    "if len(df) > 0 and len(compatible_files) > 0:\n",
    "    test_query = \"What are the main topics covered in these documents?\"\n",
    "    \n",
    "    print(f\"Test query: {test_query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    response = gfs.query_with_file_search(\n",
    "        query=test_query,\n",
    "        store_names=[store.name],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResponse:\\n{response.text}\")\n",
    "    \n",
    "    # Check for citations\n",
    "    citations = gfs.extract_citations(response)\n",
    "    if citations:\n",
    "        print(\"\\n[Sources cited from documents]\")\n",
    "    else:\n",
    "        print(\"\\n[No citations found]\")\n",
    "else:\n",
    "    print(\"Skipping test query - no documents uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "**Completado**:\n",
    "- Creado el almacén de búsqueda de archivos GFS\n",
    "- Cargados documentos compatibles\n",
    "- Verificado el estado de indexación\n",
    "- Probada la funcionalidad básica de consulta\n",
    "\n",
    "**Próximos Pasos**:\n",
    "- Proceder a `03_gfs_experiments.ipynb` para experimentos detallados de RAG\n",
    "- Probar varios patrones de consulta\n",
    "- Medir latencia y calidad de recuperación"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
