{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom RAG Baseline Implementation\n",
    "\n",
    "This notebook implements a traditional RAG system for comparison with GFS.\n",
    "\n",
    "**Components**:\n",
    "- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)\n",
    "- **Vector DB**: ChromaDB\n",
    "- **LLM**: Gemini (same as GFS)\n",
    "\n",
    "**Objectives**:\n",
    "1. Index documents with custom chunking\n",
    "2. Run same test queries as GFS experiments\n",
    "3. Measure performance metrics\n",
    "4. Compare with GFS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from custom_rag import CustomRAG\n",
    "from data_loader import scan_documents\n",
    "from utils import load_api_key\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Custom RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "api_key = load_api_key(\"GOOGLE_API_KEY\", str(project_root / \".env\"))\n",
    "\n",
    "# Initialize custom RAG\n",
    "persist_dir = project_root / \"models\" / \"custom_rag\" / \"chroma_db\"\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rag = CustomRAG(\n",
    "    api_key=api_key,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    llm_model=\"gemini-2.0-flash-exp\",\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "print(\"Custom RAG initialized\")\n",
    "print(f\"Embedding dimension: {rag.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Collection and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection\n",
    "collection_name = \"rag_comparison_docs\"\n",
    "rag.create_collection(collection_name, recreate=True)\n",
    "\n",
    "print(f\"Collection created: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan and index documents\n",
    "data_dir = project_root / \"data\" / \"raw\"\n",
    "df = scan_documents(data_dir)\n",
    "\n",
    "print(f\"Documents found: {len(df)}\")\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Index text files\n",
    "    text_extensions = {\".txt\", \".md\"}\n",
    "    indexing_results = []\n",
    "    \n",
    "    for i, row in enumerate(df.iter_rows(named=True)):\n",
    "        file_path = Path(row[\"file_path\"])\n",
    "        \n",
    "        if file_path.suffix.lower() in text_extensions:\n",
    "            print(f\"\\nIndexing {i+1}: {file_path.name}\")\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                num_chunks = rag.index_document(\n",
    "                    file_path=file_path,\n",
    "                    chunk_size=512,\n",
    "                    overlap=50,\n",
    "                    metadata={\"file_size_mb\": row[\"size_mb\"]}\n",
    "                )\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                indexing_results.append({\n",
    "                    \"file_name\": file_path.name,\n",
    "                    \"num_chunks\": num_chunks,\n",
    "                    \"indexing_time\": elapsed,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "                \n",
    "                print(f\"  ✓ Indexed {num_chunks} chunks ({elapsed:.2f}s)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                indexing_results.append({\n",
    "                    \"file_name\": file_path.name,\n",
    "                    \"status\": \"failed\",\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                print(f\"  ✗ Failed: {e}\")\n",
    "    \n",
    "    # Save indexing results\n",
    "    results_path = project_root / \"models\" / \"custom_rag\" / \"indexing_results.json\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(indexing_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nIndexing results saved to: {results_path}\")\n",
    "else:\n",
    "    print(\"No documents to index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection stats\n",
    "stats = rag.get_stats()\n",
    "print(\"\\nCollection Statistics:\")\n",
    "print(f\"  Collection: {stats.get('collection_name')}\")\n",
    "print(f\"  Total chunks: {stats.get('total_chunks')}\")\n",
    "print(f\"  Embedding dimension: {stats.get('embedding_dimension')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load same test queries as GFS experiments\n",
    "test_queries = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"query\": \"What are the main topics covered in the documents?\",\n",
    "        \"category\": \"overview\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"query\": \"Summarize the key findings or conclusions.\",\n",
    "        \"category\": \"synthesis\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q3\",\n",
    "        \"query\": \"What specific data or statistics are mentioned?\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q4\",\n",
    "        \"query\": \"Are there any recommendations or best practices?\",\n",
    "        \"category\": \"actionable\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q5\",\n",
    "        \"query\": \"What is the weather forecast for tomorrow?\",\n",
    "        \"category\": \"out_of_domain\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Test queries: {len(test_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run queries\n",
    "results = []\n",
    "\n",
    "if stats.get('total_chunks', 0) > 0:\n",
    "    for i, test in enumerate(test_queries):\n",
    "        print(f\"\\n[{i+1}/{len(test_queries)}] {test['id']}: {test['query'][:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            result_obj = rag.query(\n",
    "                query=test[\"query\"],\n",
    "                top_k=5,\n",
    "                temperature=0.0\n",
    "            )\n",
    "            \n",
    "            metrics = result_obj[\"metrics\"]\n",
    "            \n",
    "            result = {\n",
    "                \"query_id\": test[\"id\"],\n",
    "                \"category\": test[\"category\"],\n",
    "                \"total_latency_seconds\": metrics[\"total_time\"],\n",
    "                \"retrieval_time\": metrics[\"retrieval_time\"],\n",
    "                \"generation_time\": metrics[\"generation_time\"],\n",
    "                \"num_chunks_retrieved\": metrics[\"num_chunks_retrieved\"],\n",
    "                \"response_length\": len(result_obj[\"answer\"]),\n",
    "                \"avg_distance\": np.mean(result_obj[\"distances\"]) if result_obj[\"distances\"] else None,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✓ Latency: {metrics['total_time']:.2f}s (retrieval: {metrics['retrieval_time']:.2f}s, generation: {metrics['generation_time']:.2f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"query_id\": test[\"id\"],\n",
    "                \"category\": test[\"category\"],\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"  ✗ Failed: {e}\")\n",
    "        \n",
    "        results.append(result)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"\\nQueries completed: {len(results)}\")\n",
    "else:\n",
    "    print(\"No documents indexed. Skipping queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "successful_results = [r for r in results if r[\"status\"] == \"success\"]\n",
    "\n",
    "if successful_results:\n",
    "    latencies = [r[\"total_latency_seconds\"] for r in successful_results]\n",
    "    retrieval_times = [r[\"retrieval_time\"] for r in successful_results]\n",
    "    generation_times = [r[\"generation_time\"] for r in successful_results]\n",
    "    \n",
    "    print(\"Total Latency Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(latencies):.3f}s\")\n",
    "    print(f\"  Median (P50): {np.median(latencies):.3f}s\")\n",
    "    print(f\"  P95: {np.percentile(latencies, 95):.3f}s\")\n",
    "    print(f\"  P99: {np.percentile(latencies, 99):.3f}s\")\n",
    "    \n",
    "    print(f\"\\nRetrieval Time:\")\n",
    "    print(f\"  Mean: {np.mean(retrieval_times):.3f}s\")\n",
    "    print(f\"  Median: {np.median(retrieval_times):.3f}s\")\n",
    "    \n",
    "    print(f\"\\nGeneration Time:\")\n",
    "    print(f\"  Mean: {np.mean(generation_times):.3f}s\")\n",
    "    print(f\"  Median: {np.median(generation_times):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency breakdown\n",
    "if successful_results:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Stacked bar chart for latency breakdown\n",
    "    query_ids = [r[\"query_id\"] for r in successful_results]\n",
    "    \n",
    "    ax1.bar(query_ids, retrieval_times, label=\"Retrieval\")\n",
    "    ax1.bar(query_ids, generation_times, bottom=retrieval_times, label=\"Generation\")\n",
    "    ax1.set_xlabel(\"Query ID\")\n",
    "    ax1.set_ylabel(\"Time (seconds)\")\n",
    "    ax1.set_title(\"Custom RAG Latency Breakdown\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Total latency distribution\n",
    "    ax2.hist(latencies, bins=10, edgecolor=\"black\", alpha=0.7)\n",
    "    ax2.axvline(np.median(latencies), color=\"red\", linestyle=\"--\", label=\"Median\")\n",
    "    ax2.set_xlabel(\"Total Latency (seconds)\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.set_title(\"Total Latency Distribution\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experimental results\n",
    "results_path = project_root / \"models\" / \"custom_rag\" / \"experiment_results.json\"\n",
    "\n",
    "output = {\n",
    "    \"collection_name\": collection_name,\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"llm_model\": \"gemini-2.0-flash-exp\",\n",
    "    \"collection_stats\": stats,\n",
    "    \"queries\": test_queries,\n",
    "    \"results\": results,\n",
    "    \"summary\": {\n",
    "        \"total_queries\": len(results),\n",
    "        \"successful\": len(successful_results),\n",
    "        \"failed\": len(results) - len(successful_results),\n",
    "        \"mean_total_latency\": np.mean(latencies) if successful_results else None,\n",
    "        \"median_total_latency\": np.median(latencies) if successful_results else None,\n",
    "        \"mean_retrieval_time\": np.mean(retrieval_times) if successful_results else None,\n",
    "        \"mean_generation_time\": np.mean(generation_times) if successful_results else None,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Custom RAG Baseline Completed**:\n",
    "- Indexed documents with ChromaDB\n",
    "- Ran same test queries as GFS\n",
    "- Measured latency (retrieval + generation)\n",
    "- Saved results for comparison\n",
    "\n",
    "**Next Steps**:\n",
    "- Compare GFS vs Custom RAG in `05_comparison_analysis.ipynb`\n",
    "- Analyze trade-offs (latency, cost, quality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
